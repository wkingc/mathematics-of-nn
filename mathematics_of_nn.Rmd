---
title: "The Mathematical Intuition of Deep Learning"
author: Wade Copeland
date: "`r format(as.POSIXlt(Sys.time(), tz = 'America/New_York'), '%d %B, %Y')`"
output: 
    beamer_presentation:
        theme: "Malmoe"
        colortheme: "default"
        slide_level: 1
        toc: false
        includes:
            in_header: header.tex
bibliography: ./references.bib
csl: ./csl/apa-numeric-superscript-brackets.csl
---

# Why Learn the Mathematics of Deep Learning?

<!--
- Beamer in R markdown:  https://bookdown.org/yihui/rmarkdown/beamer-presentation.html
- Beamer Theme Matrix:  https://mpetroff.net/files/beamer-theme-matrix/
-->

```{r rPrelims, eval = TRUE, echo = FALSE}
# Create a chunk option for setting the size of code.
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
    x <- def.chunk.hook(x, options)
    ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

```{python pyPrelims, eval = TRUE, echo = FALSE}
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from pathlib import Path
import math
import sympy as sym
import networkx as nx
import pandas as pd
from PIL import Image
```

\emph{\Large Learning the math behind a statistical method is like learning the secret to a magic trick.  Once you know the math, you can never be fooled by the same trick again.}

<!-- 
- (1) Each OIG is responsible for the oversight of agencies that will implement deep learning AI to make more informed and better decisions, but come with risks such as bias, privacy, lack of transparency, and ethical complexities.
- (2) Each OIG will also implement deep learning AI to improve their own efficiency of operations and the outcomes of audits, evaluations, and investigations.
    - Auditors will use deep learning to write better recommendations based on data about how clients responded to previous recommendations.
    - Evaluators will use deep learning to find evidence of systemic problems in large volumes of data.
    - Investigators will use deep learning to find leads based on patterns observed their case data.
- For these reasons, OIGs cannot afford to be ignorant about deep learning, which includes having at least an intuitive understanding of the statistical methods behind it.
-->

# The Deep in Deep Learning

- Deep learning always refers to deep neural networks.  
- A deep neural network is a neural network with more than two layers.
- ChatGPT Version 4 is reported to have 1.8 trillion parameters across 120 layers @gptleak!

<!--
- This presentation is deceptively named because I will focus on shallow neural networks with two or fewer layers, however the same concepts apply to deeper networks. 
-->

# Preliminaries

- This presentation is aimed at the someone who wants to learn about deep learning without having a background in sophisticated mathematics.
- We briefly introduce some concepts from _linear algebra_ and _differential calculus_, but will focus on an intuitive understanding.

# Matricies

- A _matrix_ is a grid of numbers with $N$ rows and $P$ columns denoted $X_{N \times P}$.
    - $X_{2 \times 2} = \begin{bmatrix} x_1 & x_2 \\ x_3 & x_4 \end{bmatrix}$ is a $2 \times 2$ matrix.
    - $X_{1 \times 1} = \begin{bmatrix} x_1 \end{bmatrix}$ is a $1 \times 1$ matrix, also called a _scalar_.
    - $X_{1 \times 2} = \begin{bmatrix} x_1 & x_2 \end{bmatrix}$ is a $1 \times 2$ matrix, also called a _vector_.
    
<!--
A matrix can be thought of as a container for data.  For example, suppose we have data on arrests.  A vector of zeros and ones could indicate if an arrest was made.  To predict arrests we would have a matrix that contains features such as economic conditions and sociodemgraphic factors.
-->

# Matrix Translation

- _Matrix translation_ is the addition/subtraction of a constant to every element of a matrix.
    - The columns of $X_{2 \times 6} = \begin{bmatrix} 0 & 0 & 1 & 1 & 0 & 0 \\  0 & 1 & 1 & 0 & 0 & 1 \end{bmatrix}$ describe a square in 2-dimensional space.  We can translate the square by $0.2$ units by adding $0.2$ to each element of $X_{2 \times 6}$.
    
<!--
- While it's obviously connived for the purpose of illustration, we can think of the rows of $X$ as being features, that taken together form a square in 2-dimensional space.
-->

```{python matrixTranslation, eval = TRUE, echo = FALSE}
X = np.array([[0, 0, 1, 1, 0, 0], [0, 1, 1, 0, 0, 1]])
B = np.array([0.2])
Ztrans = X + B
Z = Ztrans - X

translation = Path("./images/translation.png")

if not translation.is_file():
    plt.clf()
    _ = plt.grid(False)
    _ = plt.figure(facecolor='white')
    _ = plt.title("Translation of 0.2 Units")
    _ = plt.plot(X[0, :], X[1, :], color = "black", alpha = 0.5)
    _ = plt.plot(Ztrans[0, :], Ztrans[1, :], color = "red", alpha = 0.5)
    _ = plt.fill_between(X[1, :], X[0, :], color = "black", alpha = 0.1)
    _ = plt.quiver(*X[:, 0], Z[0, 0], Z[1, 0], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.quiver(*X[:, 1], Z[0, 1], Z[1,1], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.quiver(*X[:, 2], Z[0, 2], Z[1, 2], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.quiver(*X[:, 3], Z[0, 3], Z[1, 3], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.gcf().set_size_inches(7, 5)
    _ = plt.savefig("./images/translation.png", bbox_inches = "tight", pad_inches= 0, dpi = 550)
    plt.close("all")

```

```{r translationImage, echo = FALSE, fig.align = "center", dpi = 1200}
knitr::include_graphics("./images/translation.png")
```

# Matrix Transformation

- _Matrix transformation_ is achieved through the matrix-specific operation called the _dot product_ where the rows of $X$ and the columns of $Y$ are multiplied and summed.
    - The dot product of $X_{1 \times 2} \cdot Y_{2 \times 1} = \begin{bmatrix} x_1 & x_2 \end{bmatrix} \cdot \begin{bmatrix} y_1 \\ y_2 \end{bmatrix} = \begin{bmatrix} x_1 y_1 + x_2 y_2 \end{bmatrix}$
    - The dot product allows us to _scale_ and _rotate_ the feature matrix.
    
<!--
- If the number of columns in $X$ does not equal the number of rows in $Y$ then the dot product operation is not defined.
- The size of the resulting matrix is equal to the number of rows in $X$ and the number of columns in $Y$.
-->

# Matrix Transformation:  Scale

- $X_{2 \times 6}$ can be scaled by $50\%$ taking the dot product of $\begin{bmatrix} 0.5 & 0 \\ 0 & 0.5 \end{bmatrix}$ and $X_{2 \times 6}$.

```{python matrixScale, eval = TRUE, echo = FALSE}
X = np.array([[0, 0, 1, 1, 0, 0], [0, 1, 1, 0, 0, 1]])
B = np.array([0.5])
W = np.array([
    [0.5, 0],
    [0,  0.5]
])
Zscale = np.dot(W, X)
Z = Zscale - X

scale = Path("./images/scale.png")

if not scale.is_file():
    plt.clf()
    _ = plt.grid(False)
    _ = plt.figure(facecolor='white')
    _ = plt.title("50% Scale")
    _ = plt.plot(X[0, :], X[1, :], color = "black", alpha = 0.5)
    _ = plt.plot(Zscale[0, :], Zscale[1, :], color = "red", alpha = 0.5)
    _ = plt.fill_between(X[1, :], X[0, :], color = "black", alpha = 0.1)
    _ = plt.quiver(*X[:, 0], Z[0, 0], Z[1, 0], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.quiver(*X[:, 1], Z[0, 1], Z[1, 1], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.quiver(*X[:, 2], Z[0, 2], Z[1, 2], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.quiver(*X[:, 3], Z[0, 3], Z[1, 3], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.gcf().set_size_inches(7, 5)
    _ = plt.savefig("./images/scale.png", bbox_inches = "tight", pad_inches= 0, dpi = 600)
    plt.close("all")

```

```{r scaleImage, echo = FALSE, fig.align = "center", dpi = 1200}
knitr::include_graphics("./images/scale.png")
```

# Matrix Transformation:  Rotation

- $X_{2 \times 6}$ can be rotated by negative 10 degrees by taking the dot product of $\begin{bmatrix} cos(0.175) & -sin(0.175) \\ sin(0.175) & cos(0.175) \end{bmatrix}$ and $X_{2 \times 6}$.

```{python matrixRotation, eval = TRUE, echo = FALSE}
X = np.array([[0, 0, 1, 1, 0, 0], [0, 1, 1, 0, 0, 1]])
theta = math.radians(-10)
W = np.array([
    [math.cos(theta), -math.sin(theta)],
    [math.sin(theta), math.cos(theta)]
])
Zrot = np.dot(W, X)
Z = Zrot - X

rotation = Path("./images/rotation.png")

if not rotation.is_file():
    plt.clf()
    _ = plt.grid(False)
    _ = plt.figure(facecolor='white')
    _ = plt.title("Rotation of -10 Degrees")
    _ = plt.plot(X[0, :], X[1, :], color = "black", alpha = 0.5)
    _ = plt.plot(Zrot[0, :], Zrot[1, :], color = "red", alpha = 0.5)
    _ = plt.fill_between(X[1, :], X[0, :], color = "black", alpha = 0.1)
    _ = plt.quiver(*X[:, 0], Z[0, 0], Z[1, 0], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.quiver(*X[:, 1], Z[0, 1], Z[1, 1], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.quiver(*X[:, 2], Z[0, 2], Z[1, 2], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.quiver(*X[:, 3], Z[0, 3], Z[1, 3], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.gcf().set_size_inches(7, 5)
    _ = plt.savefig("./images/rotation.png", bbox_inches = "tight", pad_inches= 0, dpi = 600)
    plt.close("all")

```

```{r rotationImage, echo = FALSE, fig.align = "center", dpi = 1200}
knitr::include_graphics("./images/rotation.png")
```

# Matris Transformation:  Affine

- An _affine transformation_ of $X_{2 \times 6}$ combines _transformation_ and _translation_.  For example, combining the rotation transformation of $-10$ degrees and translation of $0.2$ units.

<!--
- We care about matrix translation and transformation because these simple operations are at the heart of deep learning and are used to create a set of features that do a good job of predicting the outcome @chollet2021deep.
-->

```{python matrixAffine, eval = TRUE, echo = FALSE}
X = np.array([[0, 0, 1, 1, 0, 0], [0, 1, 1, 0, 0, 1]])
B = np.array([[0.2], [-0.2]])
theta = math.radians(-10)
W = np.array([
    [math.cos(theta), -math.sin(theta)],
    [math.sin(theta), math.cos(theta)]
])
Zaffine = np.dot(W, X) + B
Z = Zaffine - X

affine = Path("./images/affine.png")

if not affine.is_file():
    plt.clf()
    _ = plt.title("Affine Transformation")
    _ = plt.plot(X[0, :], X[1, :], color = "black", alpha = 0.5)
    _ = plt.plot(Zaffine[0, :], Zaffine[1, :], color = "red", alpha = 0.5)
    _ = plt.fill_between(X[1, :], X[0, :], color = "black", alpha = 0.1)
    _ = plt.quiver(*X[:, 0], Z[0, 0], Z[1, 0], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.quiver(*X[:, 1], Z[0, 1], Z[1, 1], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.quiver(*X[:, 2], Z[0, 2], Z[1, 2], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.quiver(*X[:, 3], Z[0, 3], Z[1, 3], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.gcf().set_size_inches(7, 5)
    _ = plt.savefig("./images/affine.png", bbox_inches = "tight", pad_inches= 0, dpi = 600)
    plt.close("all")

```

```{r affineImage, echo = FALSE, fig.align = "center", dpi = 1200}
knitr::include_graphics("./images/affine.png")
```

# Rate of Change:  The Derivative

- A _derivative_ measures the rate of change or slope at any point denoted by $\frac{\partial y}{\partial x}$, which is read "the change in $y$ with respect to the change in $x$."

<!--
- I am going to spare you the details of how to take a derivative because it's the intuitive piece that matters.

- The derivative is the slope of the line that just touches a point, also called a tangent line.  
- The plot shows the derivative of the parabola at (0.5, 0.25) and (0,0).
    - The rate of change or slope of the line at (0.5, 0.25) is 1.
    - The rate of change or slope of the line at (0,0) is 0, which is the minimum of the parabola.
    
- We care about derivatives because they give us a way to find the minimum of a function by finding where the slope is decreasing from right to left and the derivative is zero.
- This provides the basis for optimizing, for exampling finding the set of economic conditions and socioeconomic factors that minimize the number of arrests.
-->

```{python derivative, eval = TRUE, echo = FALSE}
x = sym.symbols("x")
y = (x)**2
dy = sym.diff(y, x)

x_num = np.linspace(-1, 1, 1001)

y_f = sym.lambdify(x, y, "numpy")
y_num = y_f(np.linspace(-1, 1, 1001))

dy_f = sym.lambdify(x, dy, "numpy")
dy_num = dy_f(np.linspace(-1, 1, 1001))

derivative = Path("./images/derivative.png")

if not derivative.is_file():
    plt.clf()
    _ = plt.grid(False)
    _ = plt.figure(facecolor='white')
    _ = plt.title("The Rate of Change or Slope at Every Point on the Parabola $x^2$ is $2x$")
    _ = plt.axis((-0.75, 0.75, -0.01, 0.45))
    _ = plt.plot(x_num, y_num, color = "black")
    _ = plt.plot(0.5, y_f(0.5), "s", color = "black")
    _ = plt.plot(0, y_f(0), "s", color = "black")
    _ = plt.axline((0, y_f(0)), slope = dy_f(0))
    _ = plt.axline((0.5, y_f(0.5)), slope = dy_f(0.5))
    _ = plt.annotate("$\\frac{\\partial y}{\\partial x} = 1$", (0.5 + 0.03, y_f(0.5)), size = 18)
    _ = plt.annotate("$\\frac{\\partial y}{\\partial x} = 0$", (0 + 0.01, y_f(0) + 0.015), size = 18)
    _ = plt.gcf().set_size_inches(7, 5)
    _ = plt.savefig("./images/derivative.png", bbox_inches = "tight", pad_inches= 0, dpi = 600)
    plt.close("all")

```

```{r derivativeImage, echo = FALSE, fig.align = "center", dpi = 1100}
knitr::include_graphics("./images/derivative.png")
```

# Rate of Change: The Gradient

- The _gradient_ is a generalization of the derivative that measures the rate of change or slope in every direction.  For example, if we have the surface $z = x^2 + y^2$ the slope in the $x$ direction is $\frac{\partial z}{\partial x} = 2x$ and the slope in the $y$ direction is $\frac{\partial z}{\partial y} = 2y$.

<!--
- Imagine climbing a hill at the yellow star.
    - There is the slope going up the hill which is the slope in the y-direction.
    - There is the slope going side-to-side on the hill which is the slope in the x-direction.
- At the top of the hill, the gradient is zero because there is no more up-down or side-to-side change.

- Another useful mental image is imaging a flat surface (also called a plane) drawn under where you are standing.  The gradient is the slope of that surface in every direction.

- In general a feature matrix might be very high dimensional, so graphical representations of the gradient won't be forthcoming.  As an aside, a flat surface in high dimensional space is called a hyperplane.
-->

```{python gradient, eval = TRUE, echo = FALSE}
x, y = sym.symbols('x y')

z = x**2 + y**2
z_f = sym.lambdify((x, y), z, "numpy")
z_num = z_f(np.linspace(-5, 5, 1001), np.linspace(-5, 5, 1001))

dzx = sym.diff(z, x)
dzx_f = sym.lambdify(x, dzx, "numpy")
dzx_num = dzx_f(np.linspace(-5, 5, 1001))

dzy = sym.diff(z, y)
dzy_f = sym.lambdify(y, dzy, "numpy")
dzy_num = dzy_f(np.linspace(-5, 5, 1001))

x_num, y_num = np.meshgrid(np.linspace(-5, 5, 1001), np.linspace(-5, 5, 1001))
z_num = x_num**2 + y_num**2

gradient = Path("./images/gradient.png")

if not gradient.is_file():
    plt.clf()
    fig = plt.figure(facecolor='white')
    ax = fig.add_subplot(111, projection='3d', computed_zorder=False)
    _ = plt.grid(False)
    _ = ax.plot_surface(x_num, y_num, z_num, cmap=plt.cm.viridis, linewidth=0, antialiased=True, alpha = 0.8)
    _ = ax.view_init(-140, 60)
    _ = ax.scatter(0, 0, 0, c='red', s=75, marker = "^")
    _ = ax.scatter(-3, -3, 18, c='yellow', s=100, marker = "*")
    _ = ax.set_xlabel('x')
    _ = ax.set_ylabel('y')
    _ = ax.set_zlabel('z')
    _ = plt.gcf().set_size_inches(7, 5)
    _ = plt.savefig("./images/gradient.png", bbox_inches = "tight", pad_inches= 0, dpi = 600)
    plt.close("all")

```

```{r gradientImage, echo = FALSE, fig.align = "center", dpi = 1100}
knitr::include_graphics("./images/gradient.png")
```

# Deep Learning Intuition I

<!--
- With the preliminary math done we are ready to dive into neural networks.
-->

- A neural network is an information sieve that takes as input the outcomes, and the features we want to use to predict the outcomes @chollet2021deep.
- At the top of the sieve is the most granular representation of the data, and for each successive layer down, the data becomes more and more refined, that is, better at predicting the outcome @chollet2021deep.

```{r sieve, echo = FALSE, dpi = 300, fig.align = "center", results = 'asis'}
knitr::include_graphics("./images/sieve.png", auto_pdf = TRUE)
```

<!--
- Let's go back to our example where we want to predict arrests.  The features of economic conditions and socioeconomic factors are extremely complex and multifaceted.  The goal of deep learning is to take this very complicated set of features and distill them into the data that is the best at predicting the outcome of an arrest or no arrest.
- As an aside, one of the appealing aspects of neural networks is so-called automatic feature engineering, where it doesn't matter how complicated the set of input features is, the data distillation process refines the features into their most predictive form possible.
-->

# Deep Learning Intuition II

- Consider a blue and red piece of paper crumpled together and your goal is to uncrumple them.  Each movement of your fingers to uncrumple the papers is like a layer of a neural network, until finally at the last layer the colors are fully separated @chollet2021deep.

```{r paper, echo = FALSE, dpi = 400, fig.align = "center", results = 'asis'}
knitr::include_graphics("./images/paper.PNG", auto_pdf = TRUE)
```

# Deep Learning Roadmap:  Layers

- The full process of fitting a neural network is shown below.  As a first step, we will explain the mathematical representation of the layers @chollet2021deep.

```{r deepLearningLayers, echo = FALSE, dpi = 400, fig.align = "center", results = 'asis'}
knitr::include_graphics("./images/deep_learning_layers.PNG", auto_pdf = TRUE)
```

# Representation of a Single-Layer Neural Network

- A single layer neural network is represented by the affine transformation $Y = W \cdot X + B$ @berner2021modern.

<!--
- In this diagram we are mapping three features to a single outcome.  For example, we might be mapping income, sex, and race to arrests made.
- W and B are called the weights and bias, respectively, which is just transformation and translation of the feature matrix into a form that allows us to best predict the outcome.  These are the learned values that we want to optimize (e.g., using the gradient we discuss previously) to help us make the best prediction of an arrest or no arrest.
-->

```{python singleLayer, eval = TRUE, echo = FALSE}
G = nx.DiGraph([("$x_3$", "y"), ("$x_2$", "y"), ("$x_1$", "y")])
left_nodes = ["$x_3$", "$x_2$", "$x_1$"]
right_nodes = ["y"]

pos = {n: (0, i) for i, n in enumerate(left_nodes)}
pos.update({n: (2, i + 1) for i, n in enumerate(right_nodes)})

options = {
    "font_size": 36,
    "node_size": 10000,
    "node_color": "white",
    "edgecolors": "black",
    "linewidths": 5,
    "width": 5,
}

one_layer = Path("./images/one_layer.png")

if not one_layer.is_file():
    plt.clf()
    _ = plt.figure(figsize = (14.5, 8))
    _ = nx.draw_networkx(G, pos, **options)
    _ = plt.margins(0.05, 0.1)
    _ = plt.axis("off")
    _ = plt.text(0.3, 2, "$Y_{1 \\times N} = W^{(1)}_{1 \\times 3} \\cdot X_{3 \\times N} + B^{(1)}_{1 \\times 1}$", fontsize = 35, bbox = dict(boxstyle='round', facecolor='wheat', alpha=1))
    _ = plt.savefig("./images/one_layer.png", bbox_inches = "tight", pad_inches= 0)
    plt.close("all")

```

```{r oneLayerImage, echo = FALSE, fig.align = "center", dpi = 400}
knitr::include_graphics("./images/one_layer.png")
```

# Representation of a Two-Layer Neural Network

- A two layer neural network includes a layer of _neurons_ between the features and the outcome.  These are called _hidden neurons_ because they are not observed in the input data @berner2021modern.

<!--
- In this case, instead of mapping income, sex, and race directly to the outcome, we instead map them to a layer that is a sieve in the information distillation process described before.  The "purified" data are then mapped back to the outcome.
- Each layer has its own independent set of weights.
- Finally a sneaky little function called Rho enters at the second layer, which we will discuss next.
-->

```{python twoLayer, eval = TRUE, echo = FALSE}
G = nx.DiGraph([
    ("$x_3$", "$h_1$"), ("$x_3$", "$h_2$"), ("$x_3$", "$h_3$"), ("$x_3$", "$h_4$"),
    ("$x_2$", "$h_1$"), ("$x_2$", "$h_2$"), ("$x_2$", "$h_3$"), ("$x_2$", "$h_4$"),
    ("$x_1$", "$h_1$"), ("$x_1$", "$h_2$"), ("$x_1$", "$h_3$"), ("$x_1$", "$h_4$"),
    ("$h_1$", "$\\varrho(h_1)$"), ("$h_2$", "$\\varrho(h_2)$"), ("$h_3$", "$\\varrho(h_3)$"), ("$h_4$", "$\\varrho(h_4)$"),
    ("$\\varrho(h_1)$", "y"), ("$\\varrho(h_2)$", "y"), ("$\\varrho(h_3)$", "y"), ("$\\varrho(h_4)$", "y")
])
left_nodes = ["$x_3$", "$x_2$", "$x_1$"]
middle_nodes_1 = ["$h_4$", "$h_3$", "$h_2$", "$h_1$"]
middle_nodes_2 = ["$\\varrho(h_4)$", "$\\varrho(h_3)$", "$\\varrho(h_2)$", "$\\varrho(h_1)$"]
right_nodes = ["y"]

pos = {n: (0, i + 0.5) for i, n in enumerate(left_nodes)}
pos.update({n: (1.2, i) for i, n in enumerate(middle_nodes_1)})
pos.update({n: (1.5, i) for i, n in enumerate(middle_nodes_2)})
pos.update({n: (2.8, i + 1.5) for i, n in enumerate(right_nodes)})

options = {
    "font_size": 36,
    "node_size": 10000,
    "node_color": "white",
    "edgecolors": "black",
    "linewidths": 5,
    "width": 5,
}

two_layer = Path("./images/two_layer.png")

if not two_layer.is_file():
    plt.clf()
    _ = plt.figure(figsize = (29, 12))
    _ = nx.draw_networkx(G, pos, **options)
    _ = plt.margins(0, 0.09)
    _ = plt.axis("off")
    _ = plt.text(0.01, 3.1, "$h^{(1)}_{4 \\times N} = W^{(1)}_{4 \\times 3} \\cdot X_{3 \\times N} + B^{(1)}_{1 \\times 4}$", fontsize = 40, bbox = dict(boxstyle='round', facecolor='wheat', alpha=1))
    _ = plt.text(1.65, 3.1, "$Y_{1 \\times N} = W^{(2)}_{1 \\times 4} \\cdot \\varrho(h^{(1)}_{4 \\times N}) + B^{(2)}_{1 \\times 1}$", fontsize = 40, bbox = dict(boxstyle='round', facecolor='wheat', alpha=1))
    _ = plt.savefig("./images/two_layer.png", bbox_inches = "tight", pad_inches= 0)
plt.close("all")

```

```{r twoLayerImage, echo = FALSE, fig.align = "left", dpi = 520}
knitr::include_graphics("./images/two_layer.png")
```

# What Is $\varrho(h)$ and Why Do We Need It?

- The original affine transformation that defined each layer is linear, meaning that the output falls along a line.
- The function $\varrho(h)$, also called an _activation function_, is a special kind of transformation called a non-linear transformation that allows the layer to take on many forms that are not linear @berner2021modern.
- Without an activation function between layers, a deep neural network is actually a single-layer neural network in disguise @chollet2021deep!

<!--
- Because the activation function is non-linear it allows us to model very complex data representations @chollet2021deep.

- Proof that a two layer neural network without an activation function is a single layer neural network in disguise.  To convert the LaTeX below to an image put it here:  https://latex2image.joeraut.com/.

Y_{1 \times N} = \\ 
W^{(2)}_{1 \times 4} \cdot h^{(1)}_{4 \times N} + B^{(2)}_{1 \times 1} = \\
W^{(2)}_{1 \times 4} \cdot (W^{(1)}_{4 \times 3} \cdot X_{3 \times N} + B^{(1)}_{1 \times 4}) + B^{(2)}_{1 \times 1} = \\
(W^{(2)}_{1 \times 4} \cdot W^{(1)}_{4 \times 3}) \cdot X_{3 \times N} + (W^{(2)}_{1 \times 4} B^{(1)}_{1 \times 4} + B^{(2)}_{1 \times 1})
-->

# Rectified Linear Unit (ReLU)

- A common activation function is the _ReLU_ transformation which replaces all negative values with 0 @berner2021modern.

```{python matrixRelu, eval = TRUE, echo = FALSE}
X = np.array([[0, 0, 1, 1, 0, 0], [0, 1, 1, 0, 0, 1]])
B = np.array([[0.2], [-0.2]])
theta = math.radians(-10)
W = np.array([
    [math.cos(theta), -math.sin(theta)],
    [math.sin(theta), math.cos(theta)]
])
Zaffine = np.dot(W, X) + B
Zaffine[Zaffine < 0] = 0

Z = Zaffine - X

relu = Path("./images/relu.png")

if not affine.is_file():
    plt.clf()
    _ = plt.grid(False)
    _ = plt.figure(facecolor='white')
    _ = plt.title("ReLU Transformation")
    _ = plt.plot(X[0, :], X[1, :], color = "black", alpha = 0.5)
    _ = plt.plot(Zaffine[0, :], Zaffine[1, :], color = "red", alpha = 0.5)
    _ = plt.fill_between(X[1, :], X[0, :], color = "black", alpha = 0.1)
    _ = plt.quiver(*X[:, 0], Z[0, 0], Z[1, 0], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.quiver(*X[:, 1], Z[0, 1], Z[1, 1], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.quiver(*X[:, 2], Z[0, 2], Z[1, 2], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.quiver(*X[:, 3], Z[0, 3], Z[1, 3], angles='xy', scale_units='xy', scale=1., color="black", width = 0.006, alpha = 1)
    _ = plt.gcf().set_size_inches(7, 5)
    _ = plt.savefig("./images/relu.png", bbox_inches = "tight", pad_inches= 0, dpi = 600)
    plt.close("all")

```

```{r reluImage, echo = FALSE, fig.align = "center", dpi = 1200}
knitr::include_graphics("./images/relu.png")
```

# Deep Learning Roadmap:  Loss

- After a neural network is fit, we need a way to measure how good it is at predicting the outcome of interest.  This is called a _loss function_ @chollet2021deep.

<!-- 
- For example, suppose I know that the average height of men in the U.S. 5'9 and my height is 6'0, then absolute loss in the prediction accuracy for me is 1'0.
-->

```{r deepLearningLoss, echo = FALSE, dpi = 400, fig.align = "center", results = 'asis'}
knitr::include_graphics("./images/deep_learning_loss.PNG", auto_pdf = TRUE)
```

# Measures of Loss

- For a numeric outcome a common measure of loss is the _mean squared error_, which measures the average of the squared deviations from the observed outcomes and our predictions of them @fox2008applied.

\begin{equation*}
e = \frac{1}{n}\sum_{i=1}^{n}(y - y^{\prime})^2
\end{equation*}

<!--
- Going back to the same example, if I am 6'0 then the MSE for me is (5'9-6'0)^2 = 1.
-->

- There are many other measure of loss, such as _categorical cross entropy_, which can be used to measure loss when the outcome is categorical @chollet2021deep.

# Forward Pass I

- Consider the most basic of neural networks, a single feature used to predict a single outcome. <!-- For example, Y could be the length of a slug and X the amount of slime is produces. -->
- The _forward pass_ calculates the predictions of the outcome from the input data using the values of $W$ and $B$ @chollet2021deep. <!-- The initial values of W and B are usually random, so we don't expect the initial model to do well at predicting slug slime production. -->

```{python simplenn, eval = TRUE, echo = FALSE}
G = nx.DiGraph([("$x_1$", "y")])
left_nodes = ["$x_1$"]
right_nodes = ["y"]

pos = {n: (0, i) for i, n in enumerate(left_nodes)}
pos.update({n: (2, i) for i, n in enumerate(right_nodes)})

options = {
    "font_size": 36,
    "node_size": 10000,
    "node_color": "white",
    "edgecolors": "black",
    "linewidths": 5,
    "width": 5,
}

simple_nn = Path("./images/simple_nn.png")

if not one_layer.is_file():
    plt.clf()
    _ = plt.figure(figsize = (14.5, 8))
    _ = nx.draw_networkx(G, pos, **options)
    _ = plt.margins(0.05, 0.1)
    _ = plt.axis("off")
    _ = plt.text(0.3, 0.001, "$Y_{1 \\times N} = W^{(1)}_{1 \\times 1} \\cdot X_{1 \\times N} + B^{(1)}_{1 \\times 1}$", fontsize = 35, bbox = dict(boxstyle = "round", facecolor = "wheat", alpha=1))
    _ = plt.savefig("./images/simple_nn.png", bbox_inches = "tight", pad_inches= 0, dpi = 600)
    plt.close("all")

```

```{r simplennImage, echo = FALSE, fig.align = "center", dpi = 1700}
knitr::include_graphics("./images/simple_nn.png")
```

# Forward Pass II

- Let $Y = \begin{bmatrix} 0.2 & 0.25 & 0.4 & 0.7 \end{bmatrix}$, $X = \begin{bmatrix} 0.1 & 0.3 & 0.6 & 0.7 \end{bmatrix}$,  $W = \begin{bmatrix} 0.5 \end{bmatrix}$, $B = \begin{bmatrix} 0 \end{bmatrix}$.
- The initial predictions from the first forward pass are $Y^{\prime} = W \cdot X + B = \begin{bmatrix} 0.5 \end{bmatrix} \cdot \begin{bmatrix} 0.2 & 0.25 & 0.4 & 0.7 \end{bmatrix} + \begin{bmatrix} 0 \end{bmatrix} = \begin{bmatrix} 0.05 & 0.15 & 0.3 & 0.35 \end{bmatrix}$.
- The calculated loss using the MSE is $e = \frac{1}{4}((0.2 - 0.05)^2 + (0.25 - 0.15)^2 + (0.4 - 0.3)^2 + (0.7 - 0.35)^2) = 0.0412$.

<!--
- Example from:  https://towardsdatascience.com/training-a-neural-network-by-hand-1bcac4d82a6e.
- X and Y are made up data to illustrate how the forward pass works.
- The values of W and B are chosen randomly for the first forward pass.
-->

# Forward Pass III

<!--
- Obviously this isn't a great fit because we randomly choose the values of W and B, so next we will discuss how to update them to make the model fit better!
-->

```{python forwardp, eval = TRUE, echo = FALSE, results = FALSE}
Y = np.array([[0.2, 0.25, 0.4, 0.7]])
X = np.array([[0.1, 0.3, 0.6, 0.7]])
W = np.array([[0.5]])
B = np.array([[0]])

Y_pred = np.dot(W, X) + B

e = 0.25*(((Y-Y_pred)**2).sum())

forwardp = Path("./images/forwardp.png")

if not forwardp.is_file():
    plt.clf()
    _ = plt.grid(False)
    _ = plt.figure(figsize = (5, 3), facecolor='white')
    _ = plt.scatter(X, Y)
    _ = plt.axline((0, B[0][0]), slope = W[0][0], color = "red")
    _ = plt.text(0.2, 0.05, "$Y = 0.5X + 0$", fontsize = 12)
    _ = plt.title(f"MSE After the First Forward Pass: {round(e, 4)}")
    _ = plt.savefig("./images/forwardp.png", bbox_inches = "tight", pad_inches= 0, dpi = 600)
    plt.close("all")

```

```{r forwardpImage, echo = FALSE, fig.align = "center", dpi = 600}
knitr::include_graphics("./images/forwardp.png")
```

# Deep Learning Roadmap:  Optimization

- The optimization step has the goal of _learning_ the values of $W$ and $B$ that minimize the MSE @chollet2021deep.

```{r deepLearningOptimizer, echo = FALSE, dpi = 400, fig.align = "center", results = 'asis'}
knitr::include_graphics("./images/deep_learning_optimizer.PNG", auto_pdf = TRUE)
```

# Gradient Descent

- Optimization is done numerically through an algorithm called _gradient descent_.
- Gradient descent works by stepping down the graph of our data in the opposite direction of the gradient @gradient_descent.

<!--
- Imagine you are a blind stick man walking down this parabola.  So that you don't accidentally miss the bottom, you will take smaller and smaller steps as you go down the hill.  This is the intuitive idea behind gradient descent.

-I hear in my mind from everyone who has taken Calculus:  You don't have to do this numerically, just take the derivative of the loss function, set it equal to zero, and solve!  While you are correct in this very simple example, for more complicated high-dimensional neural networks, deriving an analytic solution is very difficult.
-->

```{python gradientDescent, eval = TRUE, echo = FALSE, results = FALSE}
image = Image.open("./images/gradient_descent.png")

gradient_descent_title = Path("./images/gradient_descent_title.png")

if not gradient_descent_title.is_file():
    plt.clf()
    fig, ax = plt.subplots(figsize=(14, 4))
    _ = plt.imshow(image)
    _ = plt.gca().spines['top'].set_visible(False)
    _ = plt.gca().spines['right'].set_visible(False)
    _ = plt.gca().spines['bottom'].set_visible(False)
    _ = plt.gca().spines['left'].set_visible(False)
    _ = plt.xticks([])
    _ = plt.yticks([])
    _ = plt.title("The Gradient Descent Algorithm to Find the Minimum of a Parabola")
    _ = plt.xlabel("$w$")
    _ = plt.ylabel("$e$")
    _ = plt.savefig("./images/gradient_descent_title.png", bbox_inches = "tight", pad_inches= 0, dpi = 600)
    plt.close("all")

```

```{r gradientDescentTitleImage, echo = FALSE, dpi = 950, fig.align = "center", results = 'asis'}
knitr::include_graphics("./images/gradient_descent_title.png", auto_pdf = TRUE)
```

<!--
# Gradient Descent II

- In general, if we move $w$ and $b$ in the opposite direction of the gradient $\frac{\partial e}{\partial w}$ and $\frac{\partial e}{\partial b}$, then the MSE will decrease @chollet2021deep.
    - The updated value of $w$ is $w_u = w - \alpha(\frac{\partial e}{\partial w})$.
    - The updated value of $b$ is $b_u = b - \alpha(\frac{\partial e}{\partial b})$.

```{python updatew, eval = TRUE, echo = FALSE, results = FALSE}
x = sym.symbols("x")
y = x**2
dy = sym.diff(y, x)

x_f = sym.lambdify(x, x, "numpy")
x_num = x_f(np.linspace(-5, 5, 1001))

y_f = sym.lambdify(x, y, "numpy")
y_num = y_f(np.linspace(-5, 5, 1001))

dy_f = sym.lambdify(x, dy, "numpy")
dy_num = dy_f(np.linspace(-5, 5, 1001))

updatew = Path("./images/updatew.png")

if not updatew.is_file():
    plt.clf()
    _ = plt.grid(False)
    _ = plt.figure(facecolor='white')
    _ = plt.title("$e = w^2$ and $\\frac{\partial e}{\partial w} = 2w$: $w_{u}$ if the Learning Rate is 0.3")
    _ = plt.axis((-0.8, 0.8, -0.01, 0.5))
    _ = plt.plot(x_num, y_num, color = "black")
    _ = plt.axline((-0.5, y_f(-0.5)), slope = dy_f(-0.5), color = "blue")
    _ = plt.axline((-0.2, y_f(-0.2)), slope = dy_f(-0.2), color = "red")
    _ = plt.plot(-0.5, y_f(-0.5), "s", color = "blue")
    _ = plt.plot(-0.2, y_f(-0.2), "s", color = "red")
    _ = plt.annotate("$w = -0.5, \\frac{\\partial e}{\\partial w} = -1$", (-0.5 + 0.03, y_f(-0.5)), size = 13, color = "blue")
    _ = plt.annotate("$w_{u} = -0.2, \\frac{\\partial e}{\\partial w} = -0.4$", (-0.2 + 0.03, y_f(-0.2)), size = 13, color = "red")
    _ = plt.text(-0.25, 0.45, "$w_{u} = w - 0.3(\\frac{\\partial e}{\\partial w})$", fontsize = 13, bbox = dict(boxstyle='round', facecolor='wheat', alpha=1))
    _ = plt.xlabel("$w$")
    _ = plt.ylabel("$e$")
    _ = plt.gcf().set_size_inches(7, 5)
    _ = plt.savefig("./images/updatew.png", bbox_inches = "tight", pad_inches= 0, dpi = 600)
    plt.close("all")
```

```{r updatewImage, echo = FALSE, fig.align = "center", dpi = 1200}
knitr::include_graphics("./images/updatew.png")
```

-->

<!--
- In this example, our initial value of W is -0.5, and at this point the rate of change is -1.  After updating W, the new value is -0.2, and at this point the rate of change is -0.4.
- This step-down behavior is characteristic of a gradient descent algorithm.
- Alpha is the learning rate.  In more complex cases where there may be many minimums, setting this parameter can prevent the algorithm from getting stuck at a local minimum instead of the absolute minimum.
- I hear in my mind from everyone who has taken Calculus:  You don't have to do this numerically, just take the derivative of the loss function, set it equal to zero, and solve!  While straw man in my mind is correct in this very simple example, for more complicated neural networks, analytic solutions are not forthcoming.
-->

<!--
# A Note About Calculating Gradients

- The problem of efficiently calculating the gradient so that the loss can be minimized is solved using _backpropagation_.
    - Backpropagation works by chaining together the gradients of very simple expressions to calculate the complicated gradient for an entire neural network @chollet2021deep.
    
```{r computationGraph, echo = FALSE, fig.align = "center", dpi = 500}
knitr::include_graphics("./images/computation_graph.PNG")
```
-->

<!--
- The image below shows an example of a computation graph for a neural network.  The appearance of the gradients being chained all the way up from loss function to X is why backpropagation is sometimes called _gradient tape_ @chollet2021deep.  
- For those who are familiar with the chain rule from calculus, this will look familiar.

- Backpropagation is an elegant solution to a difficult problem and deserves more time than we have here.  For our purposes, whenever you see the word "backpropgation" think "gradient" because that is all it is.

- If we have two functions $f(x)$ and $g(x)$ and we want to calculate $\frac{\partial f(g(x))}{\partial x}$ the chain rule states $\frac{\partial f(g(x))}{\partial x} = \frac{\partial f(x)}{\partial g(x)} * \frac{\partial g(x)}{\partial x}$.
- If we have three functions $f(x)$, $g(x)$, $h(x)$ and we want to calculate $\frac{\partial f(g(h(x)))}{\partial x}$ the chain rules statues that $\frac{\partial f(g(h(x)))}{\partial x} = \frac{\partial f(x)}{\partial g(x)} * \frac{\partial g(x)}{\partial h(x)} * \frac{\partial h(x)}{\partial x}$.
- If we have four functions $f(x)$, $g(x)$, $h(x)$, $j(x)$ and we want to calculate $\frac{\partial f(g(h(j(x))))}{\partial x}$ the chain rule states $\frac{\partial f(g(h(j(x))))}{\partial x} = \frac{\partial f(x)}{\partial g(x)} * \frac{\partial g(x)}{\partial h(x)} * \frac{\partial h(x)}{\partial j(x)} * \frac{\partial j(x)}{\partial x}$.
-->

# Backward Pass

- Adjusting the values of $W$ and $B$ using gradient descent is called is called a _backward pass_ @chollet2021deep.
- Each cycle of a forward pass followed by a backward pass is called an _epoch_ @chollet2021deep.

```{python backwardPass, eval = TRUE, echo = FALSE, results = FALSE}
def Merge(dict1, dict2):
    res = {**dict1, **dict2}
    return res

class GradientDescent:
    def __init__(self, lr, x, y):
        self.lr = lr
        self.x = x
        self.y = y
        
        x1, x2, x3, x4 = sym.symbols('x1 x2 x3 x4')
        X = sym.Matrix([[x1, x2, x3, x4]])
        
        y1, y2, y3, y4 = sym.symbols('y1 y2 y3 y4')
        Y = sym.Matrix([[y1, y2, y3, y4]])
        
        w = sym.symbols('w')
        W = sym.Matrix([[w]])
        
        b = sym.symbols('b')
        B = sym.Matrix([[b, b, b, b]])
        
        K = (Y - (W*X + B))
        K_diag = sym.diag(K[0], K[1], K[2], K[3])
        
        self.MSE = ((0.25*(K*K_diag))*(sym.ones(1, 4)).T)[0]
        self.dW = 0.25*self.MSE.diff(w)
        self.dB = 0.25*self.MSE.diff(b)
        self.Wu = W[0] - self.lr*self.dW
        self.Bu = B[0] - self.lr*self.dB
        
    def values(self, w, b):
        vals = Merge(self.x, self.y)
        vals["w"] = w
        vals["b"] = b
        
        MSE = float(self.MSE.subs(vals))
        dW = float(self.dW.subs(vals))
        dB = float(self.dB.subs(vals))
        Wu = float(self.Wu.subs(vals))
        Bu = float(self.Bu.subs(vals))
        
        return({"MSE": MSE, "dW": dW, "dB": dB, "Wu": Wu, "Bu": Bu})
        
    def backward_pass(self, w, b):
        dfs = []
        ru = self.values(w, b)
        dfs.append(pd.DataFrame(ru, index = [1]))
        
        for i in range(100):
            ru = self.values(ru["Wu"], ru["Bu"])
            dfs.append(pd.DataFrame(ru, index = [1]))
        
        df = pd.concat(dfs)
        
        return(df.reset_index())

x = GradientDescent(lr = 1, x = {"x1": 0.1, "x2": 0.3, "x3": 0.6, "x4": 0.7}, y = {"y1": 0.2, "y2": 0.25, "y3": 0.4, "y4": 0.7})
res = x.backward_pass(w = 0.5, b = 0)
```

```{python epoch10, eval = TRUE, echo = FALSE, results = FALSE}
Y = np.array([[0.2, 0.25, 0.4, 0.7]])
X = np.array([[0.1, 0.3, 0.6, 0.7]])
W = np.array([[res.iloc[100]["Wu"]]])
B = np.array([[res.iloc[100]["Bu"]]])

Y_pred = np.dot(W, X) + B

e = 0.25*(((Y-Y_pred)**2).sum())

epoch100 = Path("./images/epoch100.png")

if not epoch100.is_file():
    plt.clf()
    _ = plt.grid(False)
    _ = plt.figure(facecolor='white')
    _ = plt.figure(figsize = (5, 3))
    _ = plt.scatter(X, Y)
    _ = plt.axline((0, B[0][0]), slope = W[0][0], color = "red")
    _ = plt.text(0.2, 0.2, f"$Y = {round(W[0][0], 2)}X + {round(B[0][0], 2)}$", fontsize = 12)
    _ = plt.title(f"MSE after 100 Epochs: {round(e, 4)}")
    _ = plt.savefig("./images/epoch100.png", bbox_inches = "tight", pad_inches= 0, dpi = 600)
    plt.close("all")

```

```{r epoch10Image, echo = FALSE, fig.align = "center", dpi = 700}
knitr::include_graphics("./images/epoch100.png")
```

# A Real Example:  Digit Image Classification

<!--
- The goal here not to teach you all how to code in Python, but to ground the some of the abstract concepts we have discussed in a real-world example.
-->

- One of the best use cases for neural networks is image classification.
- We will use the MNIST data set that is 70,000 images of hand-written digits 0-9 that are 28x28 pixels @chollet2015keras.  

```{python mnist, eval = TRUE, echo = FALSE, results = FALSE}
from tensorflow.keras.datasets import mnist

(train_images, train_labels), (test_images, test_labels) = (
    mnist.load_data(path='mnist.npz')
)

digit = train_images[4]

mnist = Path("./images/mnist.png")

if not mnist.is_file():
    plt.clf()
    fig = plt.figure(figsize = (5, 3), facecolor='white')
    _ = plt.grid(False)
    _ = plt.imshow(digit, cmap=plt.cm.binary)
    _ = plt.savefig("./images/mnist.png", bbox_inches = "tight", pad_inches = 0, dpi = 600)
    plt.close("all")

```

```{r mnistImage, echo = FALSE, fig.align = "center", dpi = 700}
knitr::include_graphics("./images/mnist.png")
```

# A Real Example:  Data Setup

- For analysis each image map of 28x28 pixels needs to be converted into a vector of size 28*28 = 784.
- The images are separated into a set of 60,000 images for training the model and a set of 10,000 images for testing the prediction accuracy in unseen data. \newline

```{python mnistDataSetup, eval = TRUE, echo = TRUE, results = TRUE, size = "scriptsize"}
from tensorflow.keras.datasets import mnist

(train_images, train_labels), (test_images, test_labels) = (
    mnist.load_data(path='mnist.npz')
)

train_images = train_images.reshape((60000, 28 * 28))
test_images = test_images.reshape((10000, 28 * 28))
```

# A Real Example:  Model Setup

-  The first layer of the neural network contains 512 hidden neurons and uses the ReLU activation function.
-  The output layer of the neural network is a 10-way Softmax classification layer that returns the probability that each digit belongs to a specific class. \newline

```{python mnistModelSetup, eval = TRUE, echo = TRUE, results = TRUE, size = "scriptsize"}
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(512, activation="relu"),
    layers.Dense(10, activation="softmax")
])
```

# A Real Example:  Model Compilation

- The __loss__ argument specifies how the model loss is calculated at the end of each forward pass.
- The __optimizer__ argument specifics the optimization algorithm used to update the parameter values for the backward pass.
- The __metric__ argument specifics how we will assess the model performance. \newline

```{python mnistModelCompilation, eval = TRUE, echo = TRUE, results = TRUE, size = "scriptsize"}
model.compile(
    loss="sparse_categorical_crossentropy",
    optimizer="rmsprop",
    metrics=["accuracy"])
```

<!-- 
- Sparse categorical cross entropy is a measure of loss for categorical data.
- RMSprop is a special form of gradient descent that includes _momentum_ to prevent the algorithm from getting stuck at a local minima.  Think of a ball rolling down a hill, the higher the hill the grater the momentum and more likely you might go over the top of a another hill into an even deeper valley.
- Accuracy is the percentage of accurately classified images.
-->

# A Real Example:  Model Fit

<!--
- The number of epochs is the number of forward and backward passes where we update the parameter values to get a better fit.
- The batch size means that the parameters are fit on a random subset of the input data.
-->

- In the test data, the fitted neural network correctly classified each digit greater than 95\% of the time!
- The difference in the accuracy of the training data and test data is an example of _overfitting_ @chollet2021deep. \newline

```{python mnistModelFit, eval = TRUE, echo = TRUE, results = TRUE, size = "scriptsize"}
_ = model.fit(
    train_images, train_labels, epochs=5, batch_size=128, verbose = 0)

train_acc = model.evaluate(train_images, train_labels, verbose = 0)[1]
test_acc = model.evaluate(test_images, test_labels, verbose = 0)[1]

train_acc
test_acc
```

<!--
- Overfitting:  where machine learning models tend to perform worse on new data than the data they were trained on.
- As a aside, this model using the MNIST data is better at the task of identifying handwritten digits than humans who are only correct about 90% of the time.
-->

# Summary

- This presentation peeled back the layers of what appears to be the _magic_ of deep learning.
    - We discussed how _layers_ in a neural network are like sieves for creating increasingly purified information.
    - We showed that the mathematical representation of each layer is just a combination of _matrix translation_ and _matrix transformation_.
    - We explained that the purpose of the _activation function_ applied to each layer is to detect non-linearity in the output function.
    - We showed how after each _forward pass_ from the input to the output of the model, how the parameters in each layer are updated in the _backward pass_ using _gradient descent_ and _backpropagation_.
    - We concluded by presenting an example of fitting a neural network model using real data.

<!-- 
- Why now?
    - In physics there is often a dance between theoretical findings and verification in experimental settings.  Machine learning follows a similar trajectory where new methods need to be tested with data and benchmarks to determine how well they will perform in the real world.  Many of the data sets used to test the latest developments in machine learning did exits until the 2010s @chollet2021deep.
    - Statisticians have been torturing computer hardware with machine learning algorithms for a long time, but were restricted in their ability to perform operations in parallel on clusters of CPUs.  In the 2000s though, companies such NVIDIA and AMD developed cheap single-purpose chips called GPUs that perform massively parallel matrix computations in real time.  Initially GPUs were used to render polygons in virtual worlds, but in the 2010s statisticians discovered they could use the same APIs in machine learning and drastically improve the compute time @chollet2021deep.
    - Deep neural networks with many layers are hard to train because of how complicated the gradients became.  Early optimization algorithms would produce feedback signals that faded away as the number of layers increased.  Algorithmic advances in the optimization schemes, such as backpropagation and it's variants, solved this very difficult problem @chollet2021deep.
-->

# References {.allowframebreaks}

